---
title: "Crash Course"
subtitle: "Predicting Car Insurance Claims"
author: "Jad Darwiche, Griffin Harris, Devin Lai, and Mark Leonardi"
format:
  html:
    toc: true
    embed-resources: true
    echo: false
    link-external-newwindow: true
    
execute:
  warning: false

from: markdown+emoji  
---

## GitHub Repository Link

The link to our project's GitHub Repository is [here](https://github.com/mleonardi6/301-3-final-project).

## Introduction

Car accidents are a unique threat to public health. According to the Centers for Disease Control, injuries and fatalities from motor vehicle accidents contributed to some 922 thousand "years of potential life" lost in the United States in 2020. And yet, despite this number, car accidents do not demand the same degree of attention or solemnity in the public conversation as other leaders in that statistic, like cancer, heart disease, and suicide. Car accidents, indeed, are quotidian.

Researching patterns in car accident data is thus an important challenge. Understanding what causes them, which kinds of cars and people are most likely to be involved them, and what technologies might be most useful in preventing them are steps to building a safer system of transportation and improving public health.

In this project, we offer a small contribution to that understanding. Using a dataset about car insurance policyholders obtained from a recent [Kaggle competition](https://www.kaggle.com/datasets/ifteshanajnin/carinsuranceclaimprediction-classification?select=train.csv), we explore and build predictive models to classify whether policyholders filed a claim within six months of purchasing it based on background information about the car and owner. Through the use of nine different model types, a range of hypertuning parameters, and two recipes, we were able to improve our best model's evaluation metric, ``roc_auc``.

## Data Overview

As mentioned, this data originally came from a public competition on Kaggle. Because the Kaggle testing data did not include an outcome variable, we used only the provided training set. This original full training set had nearly 60,000 observations. Initially, we planned to split these observations again, such that were about 45,000 observations in our training data and 15,000 in our testing data. However, after some early attempts to run very basic models, it became clear that such a large dataset was too computationally demanding. We re-loaded the public training set and randomly sampled a small portion of it (while ensuring proportional representation of the outcome variable), such that only 11,719 observations were left to be divided into training and testing splits.

At this stage, our data required minor cleaning and checks for missingness or imbalance in the outcome variable. The data has 43 predictor variables and one outcome variable, a binary titled `is_claim` indicating whether each observation's policyholder filed a claim. Of these 44, 27 required factoring during the initial data reading process. The remainder were numeric variables or, in a several cases, character variables (indicating, for example, the engine and fuel type). In large part because this data came from a public competition---not "real-world" application---there were few issues with messiness or missingness. The graph below shows that there were *no missing values* for any of the predictor variables in our entire dataset.

```{r}
library(tidyverse)
library(tidymodels)
library(naniar)
load("data/car_split.rda")

car_full <- car_test %>% 
  bind_rows(car_train)

gg_miss_var(car_full)
```

There was, however, a large imbalance in the outcome variable `is_claim` that required attention. A value of 0 or "no" for `is_claim` meant that the policyholder had *not* filed a claim within six months of purchasing insurance, while a 1 or "yes" indicated that a claim had been filed. Predictably, a large majority of insurance holders were not involved in accidents, as the graph below of the distribution of `is_claim` in the entire dataset shows.

```{r}
ggplot(car_full, aes(x = is_claim)) +
  geom_bar() +
  theme_minimal() +
  theme(axis.title.y = element_blank())
```

Obviously, the true goal of our model is to be able to predict when a claim *will* be filed, so even though those positive responses are small, they are essential to proper training and testing. Therefore, during our data split, we stratified by `is_claim`. We split the 11,719 observations into a testing set with 70% (or 8,203) of the observations and a training set with the remainder.

The 43 predictor variables in this dataset can be grouped into several categories. These are:

-   **Numeric variables about the car** such as its age, width, height, weight, and turning radius.

-   **Numeric variables about the policyholder's background** such as the person's age and the population density of the area that person lives in. Age, which is encoded as a normalized proportion (i.e., 28 years old is represented as .28), had a slight rightward skew, though not so severe that it required transformation.

-   **Factor variables about the car** including a long list of binaries for the presence of such features as power steering, speed alerts, front fog lights, rear window wipers, and transmission type. In some cases, there were large imbalances in these variables (nearly all cars had factor steering); in others, there was a rough balance (about half had brake assistance).

-   **Miscellaneous variables** that are either uninformative (the policy id) or unwieldy (characters describing the engine type, of which there are many).

At this stage, we conducted an extensive exploratory data analysis on the training split data to reveal relationships between predictor variables and the outcome variables that could guide feature engineering. This analysis revealed several key relationships, which are employed in the model building process.

```{r}
ggplot(car_train, aes(y = age_of_car, x = is_claim)) +
  geom_boxplot()

```

# Methods

## Models

**Nine models** were fit for this project:

-   boosted tree

-   elastic net

-   KNN

-   MARS

-   MLP (Neural Network)

-   random forest

-   SVM radial

-   SVM poly

-   Null model

-   Ensemble model


**Parameters that need to be tuned** include:

-   Elastic Net: penalty

-   K-Nearest Neighbors: number of nearest neighbors

-   Random Forest: mtry (number of predictors at each split) and min_n (minimum number of data points at a node before it can be split)

-   Boosted tree: mtry, min_n, and learn rate (weight of influence between each iteration)

-   MARS: num_terms, prod_degree (highest interaction degree)

-   MLP: hidden units and penalty

-   SVM radial: cost and rbf_sigma (radial basis function)

-   SVM poly: cost, degree, and scale factor

## Recipes

Three recipes were used in this project. The first recipe is a kitchen sink recipe, using `step_dummy()`, `step_nzv()`, `step_normalize()`, and `step_corr()`.

The second and third recipes incorporate interactions. Using EDA, we hand selected predictors that had similar correlations to the outcome variable `is_claim` and created interactions between them. Examples of these predictors include age of car and age of policy holder, as well as height and weight. The third recipe created two way interactions between all variables. Considering this created a set of over 5000 predictors, we chose to only run this recipe on models with short run times, MARS and KNN.

## Resampling

We used five cross-validation folds, stratified by the response variable `is_claim` to ensure even distribution among folds, to resample our data. Cross validation splits the training data into V folds and trains the model on V-1 of the folds, then tests the model on the last fold. The process repeats so that all folds are used for testing once, and aggregates the evaluation metric between all folds. Resampling is crucial to ensuring we produce the most accurate metrics from each model and reduces the standard error.

## Metric

We used `roc_auc` as our metric of evaluation, considering we are evaluating a classification model. `roc_auc` ranges from 0.5 to 1, with with 0.5 indicating that the model performs no better than random guessing and 1 indicating that the model has perfect performance and makes no errors in classification.

# Model Building and Selection Results

As stated previously, ``roc_auc`` is our metric of evaluation that will be used to compare models and determine the best overall model.

Below is a plot and table of the results of the best performing models using the kitchen sink recipe:

![](images/ks_plot.png)

```{r}
load("ks_recipe/ks_table.rda")

ks_table
```

For the kitchen sink recipe, the boosted tree model performed the best with an `roc_auc` of 0.630.

Below are the tuning parameters for the best performing models using the kitchen sink recipe:

```{r}
load("ks_recipe/best_params.rda")

best_params
```

The best performing kitchen sink model, boosted tree, had tuning parameters of mtry = 20, min_n = 30, and a learn rate of 0.316.

We also tuned the models using two other recipes, one including interactions with all predictors that had similar correlations to the outcome variable `is_claim`, and one that had two way interactions between all variables. Since the all interactions recipe had over 5000 predictors and would have very long computation time, only the KNN and MARS models were tuned using the all interactions recipe in order to have shorter computation time.

Below is a plot and table of the results of the best performing models using the interactions recipes:

![](images/best_plot_int.png)

```{r}
load("interactions_recipe/int_table.rda")

int_table
```

For the interactions recipes, the best performing model was once again the boosted tree model with an `roc_auc` of 0.637.

Below are the tuning parameters for the best performing models using the kitchen sink recipe:

```{r}
load("interactions_recipe/best_params_int.rda")

best_params_int
```

The best performing interactions model, boosted tree, had tuning parameters of `mtry` = 10, `min_n` = 40, and a `learn_rate` of 0.018.

Although the boosted tree model had better performance with the interactions recipe compared to with the kitchen sink recipe, all other models had either similar or slightly worse model performance with the interactions recipe than with the kitchen sink recipe. This indicates that the predictor importance of variables such as age of car, age of policy holder, height, and weight was not significant enough to cause noticeable improvement in model performance.

For both recipes, the SVM radial and KNN models had noticeably worse model performance compared to all other models. The SVM polynomial model had noticeable worse performance with the interactions recipe compared to with the kitchen sink recipe.

Out of the three recipes, the boosted tree model using the interactions recipe had the highest `roc_auc` of 0.637 and was therefore the best performing model overall. This isn't too surprising since the boosted tree model uses ensemble learning and decision trees to create a more accurate model. At the same time, it was a little surprising that the more complex models with longer run times like the SVM polynomial and radial models didn't perform up to par with models with shorter run times like MARS and neural network.

Finally, we built an ensemble model, "stacking" the five best performing models from the interactions recipe: a boosted tree, elastic net, MARS, k-nearest-neighbors, and neural network/MLP. In this process, however, we encountered several errors. 

First, the MARS model generated errors during the stacking process, which we were not able to resolve. We decided, thus, to create a second stack with only four models, excluding MARS. When building an ensemble model, it is important to achieve a balance between the number of "members" and optimal performance. The below graphs demonstrate that balance:

```{r}
load("ensemble/results/model_fit_2.rda")

autoplot(model_fit_2, type = "members")
```
As the lower graph shows, this ensemble's `roc_auc` did not measurably improve after more than about four members. Thus, the final blended and fitted stack retained three members, and it had a `penalty` of 0.01 and `mixture` of 1. Its final `roc_auc` score was .641, which would have made the ensemble our best model. 

However, other problems arose, too. First, although the model performed better than others in terms of `roc_auc`, **all of its binary predictions** were "no". In other words, the ensemble predicted the exact same outcome for every observation. Because a huge majority of test observations were, in fact, "no", this still allowed for performance that *looked* decent, even though it was not. Moreover, upon examining the weights of the ensemble model, we realized that all three of the members it kept were from the boosted tree candidate. The below graph shows this:



# Final Model Analysis

# Conclusion
