---
title: "Crash Course"
subtitle: "Predicting Car Insurance Claims"
author: "Jad Darwiche, Griffin Harris, Devin Lai, and Mark Leonardi"
format:
  html:
    toc: true
    embed-resources: true
    echo: false
    link-external-newwindow: true
    
execute:
  warning: false

from: markdown+emoji  
---


## GitHub Repository Link

The link to our project's GitHub Repository is [here](https://github.com/mleonardi6/301-3-final-project).

# Introduction

Car accidents are a unique threat to public health. According to the Centers for Disease Control, injuries and fatalities from motor vehicle accidents contributed to some 922 thousand "years of potential life" lost in the United States in 2020. And yet, despite this number, car accidents do not demand the same degree of attention or solemnity in the public conversation as other leaders in that statistic, like cancer, heart disease, and suicide. Car accidents, indeed, are quotidian.

Researching patterns in car accident data is thus an important challenge. Understanding what causes them, which kinds of cars and people are most likely to be involved them, and what technologies might be most useful in preventing them are steps to building a safer system of transportation and improving public health.

In this project, we offer a small contribution to that understanding. Using a dataset about car insurance policyholders obtained from a recent [Kaggle competition](https://www.kaggle.com/datasets/ifteshanajnin/carinsuranceclaimprediction-classification?select=train.csv), we explore and build predictive models to classify whether policyholders filed a claim within six months of purchasing it based on background information about the car and owner. Through the use of nine different model types, a range of hypertuning parameters, and two recipes, we were able to improve our best model's evaluation metric, ``roc_auc``.

# Data Overview

As mentioned, this data originally came from a public competition on Kaggle. Because the Kaggle testing data did not include an outcome variable, we used only the provided training set. This original full training set had nearly 60,000 observations. Initially, we planned to split these observations again, such that were about 45,000 observations in our training data and 15,000 in our testing data. However, after some early attempts to run very basic models, it became clear that such a large dataset was too computationally demanding. We re-loaded the public training set and randomly sampled a small portion of it (while ensuring proportional representation of the outcome variable), such that only 11,719 observations were left to be divided into training and testing splits.

At this stage, our data required minor cleaning and checks for missingness or imbalance in the outcome variable. The data has 43 predictor variables and one outcome variable, a binary titled `is_claim` indicating whether each observation's policyholder filed a claim. Of these 44, 27 required factoring during the initial data reading process. The remainder were numeric variables or, in a several cases, character variables (indicating, for example, the engine and fuel type). In large part because this data came from a public competition---not "real-world" application---there were few issues with messiness or missingness. The graph below shows that there were *no missing values* for any of the predictor variables in our entire dataset.


```{r}
library(tidyverse)
library(tidymodels)
library(naniar)
load("data/car_split.rda")

car_full <- car_test %>% 
  bind_rows(car_train)

gg_miss_var(car_full)
```


There was, however, a large imbalance in the outcome variable `is_claim` that required attention. A value of 0 or "no" for `is_claim` meant that the policyholder had *not* filed a claim within six months of purchasing insurance, while a 1 or "yes" indicated that a claim had been filed. Predictably, a large majority of insurance holders were not involved in accidents, as the graph below of the distribution of `is_claim` in the entire dataset shows.


```{r}
ggplot(car_full, aes(x = is_claim)) +
  geom_bar() +
  theme_minimal() +
  theme(axis.title.y = element_blank())
```


Obviously, the true goal of our model is to be able to predict when a claim *will* be filed, so even though those positive responses are small, they are essential to proper training and testing. Therefore, during our data split, we stratified by `is_claim`. We split the 11,719 observations into a testing set with 70% (or 8,203) of the observations and a training set with the remainder.

The 43 predictor variables in this dataset can be grouped into several categories. These are:

-   **Numeric variables about the car** such as its age, width, height, weight, and turning radius. There were no major issues with the distribution of these variables that required transformation.

-   **Numeric variables about the policyholder's background** such as the person's age and the population density of the area that person lives in. Age, which is encoded as a normalized proportion (i.e., 28 years old is represented as .28), had a slight rightward skew, though not so severe that it required transformation.

-   **Factor variables about the car** including a long list of binaries for the presence of such features as power steering, speed alerts, front fog lights, rear window wipers, and transmission type. In some cases, there were large imbalances in these variables (nearly all cars had factor steering); in others, there was a rough balance (about half had brake assistance).

-   **Miscellaneous variables** that are either uninformative (the policy id) or unwieldy (characters describing the engine type, of which there are many).

At this stage, we conducted an extensive exploratory data analysis on the training split data to reveal relationships between predictor variables and the outcome variables that could guide feature engineering. This analysis revealed several key relationships, which are employed in the model building process. We found, for example that heavier cars appeared more likely to get into car accidents, as the below graph shows:


```{r}
ggplot(car_train, aes(y = gross_weight, x = is_claim)) +
  geom_boxplot()

```

We also found that policyholders with longer policy tenures---that is, they had purchased insurance to cover a longer period of time---were more likely to file a claim within six months of that purchase, as the below graph shows.


```{r}
ggplot(car_train, aes(y = policy_tenure, x = is_claim)) +
  geom_boxplot()
```

Finally, we also see a slight positive relationship between a car's turning radius and its likelihood of being in a crash.

```{r}
ggplot(car_train, aes(y = turning_radius, x = is_claim)) +
  geom_boxplot()
```

These variables are among the strongest predictors that we discovered in our analysis. However, even these do not reveal powerful, obvious relationships with the outcome variable, but subtler ones. Later in our model-building process, we use these predictors and others we discovered as interaction terms in a recipe.

# Methods

### Models and Parameters

**Nine models** were fit for this project:

-   boosted tree

-   elastic net

-   KNN

-   MARS

-   MLP (Neural Network)

-   random forest

-   SVM radial

-   SVM poly

-   Null model

-   Ensemble model


**Parameters that need to be tuned** include:

-   Elastic Net: penalty

-   K-Nearest Neighbors: number of nearest neighbors

-   Random Forest: mtry (number of predictors at each split) and min_n (minimum number of data points at a node before it can be split)

-   Boosted tree: mtry, min_n, and learn rate (weight of influence between each iteration)

-   MARS: num_terms, prod_degree (highest interaction degree)

-   MLP: hidden units and penalty

-   SVM radial: cost and rbf_sigma (radial basis function)

-   SVM poly: cost, degree, and scale factor

### Recipes

Four recipes were used in this project. The first recipe is a kitchen sink recipe, using `step_dummy()`, `step_nzv()`, `step_normalize()`, and `step_corr()`.

The second and third recipes incorporate interactions. Using EDA, we hand selected predictors that had similar correlations to the outcome variable `is_claim` and created interactions between them. Examples of these predictors include age of car and age of policy holder, as well as height and weight. The third recipe created two way interactions between all variables. Considering this created a set of over 5000 predictors, we chose to only run this recipe on models with short run times, MARS and KNN.

The fourth recipe uses a reduced set of just seven predictor variables, described in more detail below, and interactions between some of those variables. 

### Resampling

We used five cross-validation folds, stratified by the response variable `is_claim` to ensure even distribution among folds, to resample our data. Cross validation splits the training data into V folds and trains the model on V-1 of the folds, then tests the model on the last fold. The process repeats so that all folds are used for testing once, and aggregates the evaluation metric between all folds. Resampling is crucial to ensuring we produce the most accurate metrics from each model and reduces the standard error.

### Metric

We used `roc_auc` as our metric of evaluation, considering we are evaluating a classification model. `roc_auc` ranges from 0.5 to 1, with with 0.5 indicating that the model performs no better than random guessing and 1 indicating that the model has perfect performance and makes no errors in classification.

## Model Building and Selection Results

As stated previously, `roc_auc` is our metric of evaluation that will be used to compare models and determine the best overall model.

Below is a plot and table of the results of the best performing models using the kitchen sink recipe:

![](images/ks_plot.png)


```{r}
load("ks_recipe/ks_table.rda")

ks_table
```


For the kitchen sink recipe, the boosted tree model performed the best with an `roc_auc` of 0.630.

Below are the tuning parameters for the best performing models using the kitchen sink recipe:


```{r}
load("ks_recipe/best_params.rda")

best_params
```


The best performing kitchen sink model, boosted tree, had tuning parameters of mtry = 20, min_n = 30, and a learn rate of 0.316.

We also tuned the models using two other recipes, one including interactions with all predictors that suggested strong correlations to the outcome variable `is_claim`---which predictors are discussed briefly in the "data overview" section above---and one that had two way interactions between all variables. Since the all interactions recipe had over 5000 predictors and would have very long computation time, only the KNN and MARS models were tuned using the all interactions recipe in order to have shorter computation time.

Below is a plot and table of the results of the best performing models using the interactions recipes:

![](images/best_plot_int.png)


```{r}
load("interactions_recipe/int_table.rda")

int_table
```


For the interactions recipes, the best performing model was once again the boosted tree model with an `roc_auc` of 0.637.

Below are the tuning parameters for the best performing models using the kitchen sink recipe:


```{r}
load("interactions_recipe/best_params_int.rda")

best_params_int
```


The best performing interactions model, boosted tree, had tuning parameters of `mtry` = 10, `min_n` = 40, and a `learn_rate` of 0.018.

Although the boosted tree model had better performance with the interactions recipe compared to with the kitchen sink recipe, all other models had either similar or slightly worse model performance with the interactions recipe than with the kitchen sink recipe. This indicates that the predictor importance of variables such as age of car, age of policy holder, height, and weight was not significant enough to cause noticeable improvement in model performance.

For both recipes, the SVM radial and KNN models had noticeably worse model performance compared to all other models. The SVM polynomial model had noticeable worse performance with the interactions recipe compared to with the kitchen sink recipe.

Out of the three recipes, the boosted tree model using the interactions recipe had the highest `roc_auc` of 0.637 and was therefore the best performing model overall. This isn't too surprising since the boosted tree model uses ensemble learning and decision trees to create a more accurate model. At the same time, it was a little surprising that the more complex models with longer run times like the SVM polynomial and radial models didn't perform up to par with models with shorter run times like MARS and neural network. After retuning this boosted tree model with narrowed ranges for `mtry`, `learn_rate`, and `min_n`, we did not observe significantly improved performance.

Next, we built an ensemble model, "stacking" the five best performing models from the interactions recipe: a boosted tree, elastic net, MARS, k-nearest-neighbors, and neural network/MLP. In this process, however, we encountered several errors. 

First, the MARS model generated errors during the stacking process, which we were not able to resolve. We decided, thus, to create a second stack with only four models, excluding MARS. When building an ensemble model, it is important to achieve a balance between the number of "members" and optimal performance. The below graphs demonstrate that balance:


```{r}
load("ensemble/results/model_fit_2.rda")
load("ensemble/results/stack_blend_2.rda")
library(stacks)

autoplot(model_fit_2, type = "members")
```

As the lower graph shows, this ensemble's `roc_auc` did not measurably improve after more than about four members. Thus, the final blended and fitted stack retained three members, and it had a `penalty` of 0.01 and `mixture` of 1. Its final `roc_auc` score was .641, which would have made the ensemble our best model. 

However, other problems arose, too. Upon examining the weights of the ensemble model, we realized that all three of the members it kept were from the boosted tree candidate. The below graph shows this:


```{r}

autoplot(stack_blend_2, type = "weights")
```

Thus, the ensemble model was essentially only using the boosted tree candidate, and, while it had a better `roc_auc` than other model types we had run, that was a misleading metric, since this model clearly had underlying errors that we were unable to address. For this reason, we did not select the ensemble as our final model, despite it technically performing the best in terms of our evaluation metric. 

At this stage, it appeared as though the boosted tree recipe with interactions and narrowed tuning parameters would be our best model. However, when we fit this model to the final testing data, we noticed a major problem: the model was predicting all *no*s. In other words, every observation was predicted as a 0. While this gave us a decent `roc_auc`, that metric was misleading, since the model clearly had underlying errors. In the below graph, this problem is shown: the fill in the bar chart represents actual observations, while the x-axis represents predictions; there is not a "yes" column because 0 yeses were predicted.


```{r}
load("interactions_recipe/results/boosted_retuned.rda")
load("data/filtered_interact_recipe.rda")

boosted_model <- boost_tree(mode = "classification", 
                            learn_rate = tune(), 
                            min_n = tune(),
                            mtry = tune()) %>% 
  set_engine("xgboost", importance = "impurity")

boosted_workflow <- workflow() %>% 
  add_model(boosted_model) %>% 
  add_recipe(interactions_recipe)

boosted_workflow <- boosted_workflow %>% 
  finalize_workflow(select_best(boosted_retuned, metric = "roc_auc"))

final_fit <- fit(boosted_workflow, car_train)

results <- car_test %>% 
  bind_cols(predict(final_fit, new_data = car_test)) %>% 
  select(c(is_claim, contains("pred")))


ggplot(results, aes(x = .pred_class, group = is_claim, fill = is_claim)) +
  geom_bar()

```


We believed the cause of this problem to be over-fitting, so we made several key changes to produce our final model. First, we used lasso and MARS models to do a variable reduction on our data, which we believed would reduce the problem of over-fitting and allow our model to make more realistic predictions. These key variables, pulled from the original 43 predictors, were the car's height, the policy tenure purchased, the age of the car, whether the car had a "speed alert" system, the age of the policyholder, and the car's gross weight.

Next, we re-tuned the boosted tree and set the range for the parameter `learn_rate` to be very low---lower than it had been in previous scripts---to again reduce the problem of over-fitting. 

Despite these efforts, however, the final model's binary predictions were still entirely *no*s---at first. But we noticed that the *probability* of yeses in some observations were significantly higher with this model than they had been with previous ones. In each observation, the model predicts the binary outcome with a certain degree of confidence in that outcome; it is this degree of confidence, expressed numerically, that `roc_auc` takes into account. Thus, the final step we took was to lower the threshold of that probability for the model to create a "yes" prediction, which yielded a more informative set of results.

## Final Model Analysis

Our final model was a boosted tree, trained on folded reduced-variable data and a recipe that contained interactions between key predictor terms. The `mtry` was 5, the `min_n` was 40, and the `learn_rate` was .001.

Once fitted to the full training data, this model predicted on the testing data. The final `roc_auc` score for these predictions was .643, the best score we achieved across any models. In the graph below, we show that metric with a graph of the receiver operating curve.


```{r}
load("data/car_split_mars.rda")
load("interactions_recipe/results/boosted_tuned3.rda")

boosted_model <- boost_tree(mode = "classification", 
                            learn_rate = c(0.005, 0.02), 
                            min_n = tune(),
                            mtry = tune()) %>% 
  set_engine("xgboost", importance = "impurity")

boosted_workflow <- workflow() %>% 
  add_model(boosted_model) %>% 
  add_recipe(interactions_recipe)

boosted_workflow <- boosted_workflow %>% 
  finalize_workflow(select_best(boosted_tuned_filtered, metric = "roc_auc"))

final_fit <- fit(boosted_workflow, filtered_train)

results_prob <- filtered_test %>% 
  bind_cols(predict(final_fit, new_data = filtered_test, type = "prob")) %>% 
  select(c(is_claim, contains("pred")))

results <- filtered_test %>% 
  bind_cols(predict(final_fit, new_data = filtered_test)) %>% 
  select(c(is_claim, contains("pred")))

curve <- roc_curve(
  results_prob, 
  truth = is_claim, 
  estimate = .pred_No)

autoplot(curve)
```


As discussed above, to obtain the binary predictions that we were most interested in---the above show probabilistic predictions---we had to lower the threshold for the model to predict an observation as "yes." This threshold was lowered to .471. In the two graphs below, we show our results before and after lowering that threshold. 


```{r}
load("data/final_results.rda")
library(patchwork)

plot_1 <- ggplot(results, aes(x = .pred_class)) +
  geom_bar()


plot_2 <- ggplot(results, aes(x = .pred_new)) +
  geom_bar()

plot_1 + plot_2
```

On the left, there are all "no" predictions; after lowering the threshold, some yes predictions appear on the right. This slightly improved our predictive results, but not very significantly. The confusion matrix below offers a clearer sense of the model's accuracy: 


```{r}
conf_mat(results, truth = is_claim, estimate = .pred_new)


```

Clearly, this model struggled with identifying positive cases where the policyholder had filed a claim. In only 28 cases of 236 did the model accurately predict these positives. That likely owes to several factors, the most important of them being the massive imbalance in the outcome variable described earlier, which likely skewed the model results. Moreover, we are conscious of the limited computational power available to us---a fact that also forced us to remove observations from our training set---and of the difficulty inherent to data from a well-advertised, well-sponsored public Kaggle competition. 


# Conclusion

In this project, we used data about auto insurance policyholders and their vehicles to make predictions about whether those policyholders would file a claim within six months of their insurance purchase. In the process, we investigated---if tangentially---some of the key factors that explain and could possibly help solve one of the largest issues in public health policy. Our final model's low-to-moderate predictive accuracy revealed both room for improvement but also the complexity of issues in transportation policy, vehicle design, road safety, and insurance markets. 

We used many different methods to build a strong model. Nine different model types, four different recipes, two variable reductions, and narrowed hypertuning parameters were all employed. Ultimately, our best model was a boosted tree trained on a reduced-variable recipe with interactions and optimized across three hyperparameters. This generated a `roc_auc` of .643. 

In future research, the important questions at play in this project could receive further attention. Researchers with more powerful tools and techniques could continue to investigate this data. More importantly, these findings could offer guides to new data and questions---about how age impacts driving, for example, or optimal vehicle weights for the safest conditions---that might inform the public conversation. 


