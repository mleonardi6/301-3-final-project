---
title: "Final Project Progress Memo 2"
author: "Jad Darwiche, Griffin Harris, Devin Lai, and Mark Leonardi"
format:
  html:
    toc: true
    embed-resources: true
    code-fold: show
    link-external-newwindow: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji  
---

### GitHub Link

The repository for our project is [here](https://github.com/mleonardi6/301-3-final-project).

### Recipe and Feature Engineering

We used a kitchen sink recipe with the following feature engineering steps:

-   We used `step_rm` to remove the `policy_id` variable since it's only serves as an identifier

-   We used `step_dummy` to input all the factor variables as numeric values in the models, which is necessary for models such as elastic net and support vector machine

-   We used `step_nzv` to remove near zero-variance predictors and to handle rarely occuring categories. Also, some of the variables have a large amount of factor levels

-   We used `step_normalize` to ensure that all predictors are on the same scale

-   We used `step_corr` to handle highly correlated predictor variables so that models such as logistic regression and support vector machines aren't adversely affected by them

-   We did not include imputation in our feature engineering process since missingness was not an issue in our data

### Assessment Measure

We will be using the roc_auc metric to compare performances between models and to select the best performing model. The roc_auc metric measures a model\'s ability to correctly classify positive and negative classes. It ranges from 0.5 to 1, with values closer to 1 indicating better classification and predictive performance.

Below is a table with the roc_auc and run times of all 8 models we've tuned, including the null baseline model:

```{r, echo=FALSE}
# get final model results

library(tidymodels)
library(tidyverse)
library(kableExtra)

tidymodels_prefer()

result_files <- list.files("results/", "*.rda", full.names = TRUE)

for(i in result_files){
  load(i)
}

load("data/kitchen_sink_recipe.rda")

load("data/car_folds.rda")

##########################################
# baseline/null model
null_model <- null_model(mode = "classification") %>% 
  set_engine("parsnip")

null_workflow <- workflow() %>% 
  add_model(null_model) %>% 
  add_recipe(kitchen_sink_recipe)

null_fit <- null_workflow %>% 
  fit_resamples(resamples = car_folds,
                control = control_resamples(save_pred = TRUE))

null_fit <- null_fit %>% 
  collect_metrics() %>% 
  mutate(model = "Null") %>% 
  rename(roc_auc = mean) %>% 
  filter(.metric == "roc_auc") %>% 
  select(model, roc_auc)

####################
# put all our tune_grids together
model_set <- as_workflow_set(
  "elastic_net" = elastic_net_tuned, 
  "rand_forest" = rf_tuned,
  "knn" = knn_tuned,
  "boosted_tree" = boosted_tuned,
  "neural_network" = nn_tuned,
  "svm_poly" = svm_poly_tuned,
  "svm_radial" = svm_radial_tuned,
  "mars" = mars_tuned
)

model_results <- model_set %>% 
  group_by(wflow_id) %>% 
  mutate(best = map(result, show_best, metric = "roc_auc", n = 1)) %>% 
  select(best) %>% 
  unnest(cols = c(best))

# computation time
model_times <- bind_rows(elastic_net_tictoc,
                         boosted_tictoc,
                         rf_tictoc,
                         knn_tictoc,
                         nn_tictoc,
                         svm_poly_tictoc,
                         svm_radial_tictoc,
                         mars_tictoc) %>% 
  mutate(wflow_id = c("elastic_net", 
                      "rand_forest",
                      "knn",
                      "boosted_tree",
                      "neural_network",
                      "svm_poly",
                      "svm_radial",
                      "mars"))

results_table <- merge(model_results, model_times) %>% 
  select(model, mean, runtime) %>% 
  rename(roc_auc = mean) 

results_table <- bind_rows(results_table, null_fit) 

results_table %>% 
  as_tibble() %>% 
  kbl() %>% 
  kable_styling()
```

All 8 models had a higher roc_auc than the null baseline model, which means that all 8 models performed better than the null baseline model. The SVM polynomial model had by far the longest runtime, followed by the random forest model. There were no issues that occurred when the models were fitted.
